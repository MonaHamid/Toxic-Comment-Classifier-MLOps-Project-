{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aeba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#data visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "#to avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"data/raw/train.csv\")\n",
    "test_df = pd.read_csv(\"data/raw/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8579e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_labels = train_df.columns.tolist()[2:]\n",
    "train_df[column_labels].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b409dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['toxic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split toxic and clean\n",
    "train_toxic = train_df[train_df['toxic'] == 1]\n",
    "train_non_toxic = train_df[train_df['toxic'] == 0]\n",
    "\n",
    "#  Undersample clean class to match toxic count\n",
    "train_non_toxic_sampled = train_non_toxic.sample(n=len(train_toxic), random_state=42)\n",
    "\n",
    "# : Combine both and shuffle\n",
    "train_balanced = pd.concat([train_toxic, train_non_toxic_sampled], axis=0)\n",
    "train_balanced = train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#  Confirm balance\n",
    "print(train_balanced['toxic'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_balanced.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a58148",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    train_balanced['comment_text'],  # or dataframe['comment_text'] if it's the same\n",
    "    train_balanced.iloc[:, 2:],      # all 6 label columns\n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test set into 50% validation and 50% final test\n",
    "val_texts, final_test_texts, val_labels, final_test_labels = train_test_split(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfbf2f",
   "metadata": {},
   "source": [
    "Tokenization and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_encode(tokenizer, comments, labels, max_length=128):\n",
    "    # Tokenize the list of comments at once (batch encoding is more efficient)\n",
    "    encoding = tokenizer(\n",
    "        comments.tolist(),                      # Ensure it's a list of strings\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',                   # Use padding parameter instead of pad_to_max_length\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Convert labels to torch tensors\n",
    "    labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "    return encoding['input_ids'], encoding['attention_mask'], labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb06a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "#  Initialize Model (6 labels for multi-label classification)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=6,\n",
    "    problem_type=\"multi_label_classification\"  # Important for multi-label setup\n",
    ")\n",
    "\n",
    "# Device Setup (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "#  Tokenize and Encode Each Dataset\n",
    "train_input_ids, train_attention_masks, train_labels_tensor = tokenize_and_encode(\n",
    "    tokenizer, train_texts, train_labels)\n",
    "\n",
    "val_input_ids, val_attention_masks, val_labels_tensor = tokenize_and_encode(\n",
    "    tokenizer, val_texts, val_labels)\n",
    "\n",
    "test_input_ids, test_attention_masks, test_labels_tensor = tokenize_and_encode(\n",
    "    tokenizer, test_texts, test_labels)\n",
    "\n",
    "# Check Shapes\n",
    "print('Training Comments :', train_texts.shape)\n",
    "print('Input Ids         :', train_input_ids.shape)\n",
    "print('Attention Mask    :', train_attention_masks.shape)\n",
    "print('Labels            :', train_labels_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdcf085",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced.to_csv(\"train_balanced.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b896a491",
   "metadata": {},
   "source": [
    "Creating Pytorch Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fa99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Train DataLoader\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Validation DataLoader\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test DataLoader\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f791ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View one batch of training data\n",
    "print('Batch Size :', train_loader.batch_size)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print('Each Input ids shape      :', batch[0].shape)  # (batch_size, sequence_length)\n",
    "print('Input ids [0]             :\\n', batch[0][0])\n",
    "print('Decoded text [0]          :\\n', tokenizer.decode(batch[0][0], skip_special_tokens=True))\n",
    "print('Corresponding Attention Mask:\\n', batch[1][0])\n",
    "print('Corresponding Label [0]   :', batch[2][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44813af",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12da2a2",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to Train the Model\n",
    "# def train_model(model, train_loader, optimizer, device, num_epochs):\n",
    "#     # Loop through the specified number of epochs\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Set the model to training mode\n",
    "#         model.train()\n",
    "#         # Initialize total loss for the current epoch\n",
    "#         total_loss = 0\n",
    "\n",
    "#         # Loop through the batches in the training data\n",
    "#         for batch in train_loader:\n",
    "#             input_ids, attention_mask, labels = [t.to(device) for t in batch]\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             outputs = model(\n",
    "#                 input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#             loss = outputs.loss\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         model.eval()  # Set the model to evaluation mode\n",
    "#         val_loss = 0\n",
    "\n",
    "#         # Disable gradient computation during validation\n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 input_ids, attention_mask, labels = [\n",
    "#                     t.to(device) for t in batch]\n",
    "\n",
    "#                 outputs = model(\n",
    "#                     input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#                 loss = outputs.loss\n",
    "#                 val_loss += loss.item()\n",
    "#         # Print the average loss for the current epoch\n",
    "#         print(\n",
    "#             f'Epoch {epoch+1}, Training Loss: {total_loss/len(train_loader)},Validation loss:{val_loss/len(val_loader)}')\n",
    "\n",
    "\n",
    "# # Call the function to train the model\n",
    "# train_model(model, train_loader, optimizer, device, num_epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
