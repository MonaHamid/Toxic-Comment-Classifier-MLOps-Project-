{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82yhG9Vy9VOM",
    "outputId": "72260ef2-4fb7-4469-910d-9fc60706dd12"
   },
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib seaborn wordcloud pillow matplotlib-venn spacy nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pyN4UpC90LV",
    "outputId": "2304cce8-8c8c-49a3-bfdc-f54c1200c09a"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmx7yGB39P_1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#misc\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "#stats\n",
    "from scipy import sparse\n",
    "import scipy.stats as ss\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud ,STOPWORDS\n",
    "from PIL import Image\n",
    "import matplotlib_venn as venn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#misc\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "#stats\n",
    "from scipy import sparse\n",
    "import scipy.stats as ss\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud ,STOPWORDS\n",
    "from PIL import Image\n",
    "import matplotlib_venn as venn\n",
    "\n",
    "#nlp\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "#settings\n",
    "start_time=time.time()\n",
    "color = sns.color_palette()\n",
    "sns.set_style(\"dark\")\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "K1iuncmG9yXW",
    "outputId": "fe8d1482-8076-4744-bd51-456d8a786baf"
   },
   "outputs": [],
   "source": [
    "file_path = '/content/drive/MyDrive/toxic_comment/train.csv'\n",
    "train = pd.read_csv(file_path)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "y93HElOD9Q1j",
    "outputId": "06fc5c33-154b-4d7c-fbfc-eb0aa3cb36b9"
   },
   "outputs": [],
   "source": [
    "file_path = '/content/drive/MyDrive/toxic_comment/test.csv'\n",
    "test= pd.read_csv(file_path)\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "FudhcRcS-EVJ",
    "outputId": "0f02b8d2-03bb-49b3-c8e1-32b9ba27618e"
   },
   "outputs": [],
   "source": [
    "\n",
    "label_col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "if 'is_clean' not in train.columns:\n",
    "    train['is_clean'] = (train[label_col].sum(axis=1) == 0).astype(int)\n",
    "\n",
    "# Total characters\n",
    "train['total_len'] = train['comment_text'].apply(len)\n",
    "test['total_len'] = test['comment_text'].apply(len)\n",
    "\n",
    "# Sentence count\n",
    "train['sent_count'] = train[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\", str(x))) + 1)\n",
    "test['sent_count'] = test[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\", str(x))) + 1)\n",
    "\n",
    "# Word count\n",
    "train['word_count'] = train[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "test['word_count'] = test[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Plot KDEs\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.suptitle(\"Are longer comments more toxic?\", fontsize=18)\n",
    "\n",
    "# Characters\n",
    "plt.subplot(131)\n",
    "sns.kdeplot(train[train.is_clean == 0]['total_len'], label=\"UnClean\", shade=True, color='r')\n",
    "sns.kdeplot(train[train.is_clean == 1]['total_len'], label=\"Clean\")\n",
    "plt.legend()\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.xlabel('# of Chars', fontsize=12)\n",
    "\n",
    "# Words\n",
    "plt.subplot(132)\n",
    "sns.kdeplot(train[train.is_clean == 0]['word_count'], label=\"UnClean\", shade=True, color='r')\n",
    "sns.kdeplot(train[train.is_clean == 1]['word_count'], label=\"Clean\")\n",
    "plt.legend()\n",
    "plt.xlabel('# of Words', fontsize=12)\n",
    "\n",
    "# Sentences\n",
    "plt.subplot(133)\n",
    "sns.kdeplot(train[train.is_clean == 0]['sent_count'], label=\"UnClean\", shade=True, color='r')\n",
    "sns.kdeplot(train[train.is_clean == 1]['sent_count'], label=\"Clean\")\n",
    "plt.legend()\n",
    "plt.xlabel('# of Sentences', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8jIpl0YElqR"
   },
   "source": [
    "most comments are having less than 25 sentences & less than 250 words\n",
    "\n",
    "unclean comments are having more no.of words in less no.of sentences.\n",
    "\n",
    "The distrubution plots of clean & unclean of all three plots are very much overlapping with each others, indicating these features are going to be less significant in differentiating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "_KLqmgpxEmeX",
    "outputId": "70ed3b8b-ea7b-472e-a838-ae0d9d7e0189"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "# Ensure is_clean exists (0 = toxic, 1 = clean)\n",
    "label_col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "if 'is_clean' not in train.columns:\n",
    "    train['is_clean'] = (train[label_col].sum(axis=1) == 0).astype(int)\n",
    "\n",
    "# Capital letters count\n",
    "train['capitals'] = train['comment_text'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\n",
    "test['capitals'] = test['comment_text'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\n",
    "\n",
    "# Punctuation count\n",
    "train['punct_count'] = train['comment_text'].apply(lambda x: sum(1 for c in str(x) if c in string.punctuation))\n",
    "test['punct_count'] = test['comment_text'].apply(lambda x: sum(1 for c in str(x) if c in string.punctuation))\n",
    "\n",
    "# Smilies count\n",
    "smilies = (':-)', ':)', ';-)', ';)')\n",
    "train['smilies_count'] = train['comment_text'].apply(lambda comment: sum(str(comment).count(s) for s in smilies))\n",
    "test['smilies_count'] = test['comment_text'].apply(lambda comment: sum(str(comment).count(s) for s in smilies))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.suptitle(\"Does the Presence of Special Characters Vary with Toxicity?\", fontsize=18)\n",
    "\n",
    "# Capitals\n",
    "plt.subplot(131)\n",
    "sns.kdeplot(train[train.is_clean == 0]['capitals'], label=\"Toxic\", shade=True, color='r')\n",
    "sns.kdeplot(train[train.is_clean == 1]['capitals'], label=\"Clean\")\n",
    "plt.legend()\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.xlabel('# of Capital Letters', fontsize=12)\n",
    "\n",
    "# Punctuations\n",
    "plt.subplot(132)\n",
    "sns.kdeplot(train[train.is_clean == 0]['punct_count'], label=\"Toxic\", shade=True, color='r')\n",
    "sns.kdeplot(train[train.is_clean == 1]['punct_count'], label=\"Clean\")\n",
    "plt.legend()\n",
    "plt.xlabel('# of Punctuations', fontsize=12)\n",
    "\n",
    "# Smilies\n",
    "plt.subplot(133)\n",
    "sns.kdeplot(train[train.is_clean == 0]['smilies_count'], label=\"Toxic\", shade=True, color='r')\n",
    "sns.kdeplot(train[train.is_clean == 1]['smilies_count'], label=\"Clean\")\n",
    "plt.legend()\n",
    "plt.xlabel('# of Smilies', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyXfW4K4FOsk"
   },
   "source": [
    "presence of captial letters is more in case of unclean comments, but the distrbutions are overlapping making it a difficult feature for models to extract information.\n",
    "\n",
    "most of the clean comments are having punctuations less than 100 while for unclean comments it spread to max of 5000 punctuations.\n",
    "\n",
    "no.of smilies in unclean v/s clean comments is very much similar and unclean comments are having more comments with no.of smilies = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "Uj78xr1rFScW",
    "outputId": "10029d6b-243c-4c31-ed9b-8e6a4008a9d2"
   },
   "outputs": [],
   "source": [
    "# Unique word count\n",
    "train['unique_word_count'] = train[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test['unique_word_count'] = test[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# Unique ratio\n",
    "train['unique_word_percent'] = (train['unique_word_count'] / train['word_count']) * 100\n",
    "test['unique_word_percent'] = (test['unique_word_count'] / test['word_count']) * 100\n",
    "\n",
    "# ---------- Plotting ------------\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.suptitle(\"Comments with Less-Unique-Words (Spam) vs Toxicity?\", fontsize=18)\n",
    "\n",
    "# KDE plot for unique word percentage\n",
    "plt.subplot(121)\n",
    "plt.title(\"% of Unique Words in Comments\")\n",
    "sns.kdeplot(train[train.is_clean == 0]['unique_word_percent'], label=\"Toxic\", shade=True, color='r')\n",
    "sns.kdeplot(train[train.is_clean == 1]['unique_word_percent'], label=\"Clean\")\n",
    "plt.legend()\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.xlabel('Percent Unique Words', fontsize=12)\n",
    "\n",
    "# Violin plot for comments with <25% unique words\n",
    "plt.subplot(122)\n",
    "sns.violinplot(\n",
    "    y='unique_word_count', x='is_clean',\n",
    "    data=train[train['unique_word_percent'] < 25],\n",
    "    split=True, inner=\"quart\"\n",
    ")\n",
    "plt.xlabel('is_Clean', fontsize=12)\n",
    "plt.ylabel('# of Unique Words', fontsize=12)\n",
    "plt.title(\"# Unique Words vs Toxicity\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTwgksw-F334"
   },
   "source": [
    "There is a wide spread area for unclean points in the unique word percentage range of 1-10%, Interesting there are clean comments as well with lesser number of unique words.\n",
    "\n",
    "This feature seems carry some significance especially incase of sentences with less unique words.\n",
    "\n",
    "lets once see how text in clean-spam & unclean-spam comments look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lvpy_-9F8NQ",
    "outputId": "01da632b-8c21-4096-a510-034d066355d1"
   },
   "outputs": [],
   "source": [
    "## lets have a look how clean & unclean spam comment looks like\n",
    "\n",
    "print(\"Clean Spam example:\")\n",
    "print(train[train['unique_word_percent'] < 10][train['is_clean'] == 1].comment_text.iloc[3])\n",
    "print('-'*50)\n",
    "print(\"Toxic Spam example:\")\n",
    "print(train[train['unique_word_percent'] < 10][train['is_clean'] == 0].comment_text.iloc[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nllenlxGc3t"
   },
   "source": [
    "we definitely need to penalize the over repeating words (or) there will be chance for models to learn as words like 'George Bush' as indicator as toxicity. so comments with lesser unique words should be preprocessed carefully.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8sbhPqD-Ksr"
   },
   "source": [
    "##Classical models (TF-IDF) cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlMa3KOG-Jkt"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "CONTRACTIONS = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"i'm\": \"i am\", \"it's\": \"it is\",\n",
    "    \"don't\": \"do not\", \"didn't\": \"did not\", \"you're\": \"you are\",\n",
    "    \"they're\": \"they are\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
    "    # add more as needed\n",
    "}\n",
    "def expand_contractions(text):\n",
    "    def replace(match):\n",
    "        return CONTRACTIONS.get(match.group(0).lower(), match.group(0))\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, CONTRACTIONS.keys())) + r')\\b', flags=re.IGNORECASE)\n",
    "    return pattern.sub(replace, text)\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    text = expand_contractions(text)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)              # remove urls\n",
    "    text = re.sub(r'\\d+', ' ', text)                           # remove numbers (optional)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()                   # normalize spaces\n",
    "    # remove stopwords\n",
    "    tokens = [t for t in text.split() if t not in eng_stopwords]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7uELAke-Zyn",
    "outputId": "201ae0b1-fc7e-4512-840c-27e34b365651"
   },
   "outputs": [],
   "source": [
    "\n",
    "train['clean_text'] = train['comment_text'].apply(clean_text)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=50_000,\n",
    "\n",
    "    stop_words=None\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(train['clean_text'])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(\"TF‑IDF shape:\", X_tfidf.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrQnbBBX9bvA"
   },
   "source": [
    "## TF-IDF Vectorizer\n",
    "TF - Term Frequency -- Count of the words(Terms) in the text corpus (same of Count Vect)\n",
    "IDF - Inverse Document Frequency -- Penalizes words that are too frequent. We can think of this as regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ziirk_poHf_i"
   },
   "outputs": [],
   "source": [
    "def get_topn_tfidf_feat_byClass(X_tfidf, y_train, feature_names, labels, topn):\n",
    "    feat_imp_dfs = {}\n",
    "\n",
    "    for label in labels:\n",
    "        # Get indices of rows where label is true\n",
    "        label_ids = y_train.index[y_train[label] == 1]\n",
    "        # Get subset of rows\n",
    "        label_rows = X_tfidf[label_ids].toarray()\n",
    "        # Calculate mean feature importance\n",
    "        feat_imp = label_rows.mean(axis=0)\n",
    "        # Sort by column dimension and get topn feature indices\n",
    "        topn_ids = np.argsort(feat_imp)[::-1][:topn]\n",
    "        # Combine tfidf value with feature name\n",
    "        topn_features = [(feature_names[i], feat_imp[i]) for i in topn_ids]\n",
    "        # Save to dataframe\n",
    "        topn_df = pd.DataFrame(topn_features, columns=['word_feature', 'tfidf_value'])\n",
    "        feat_imp_dfs[label] = topn_df\n",
    "\n",
    "    return feat_imp_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jn4mKiC6Wknu"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 1),  # unigrams\n",
    "    max_features=50000,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit and transform the training text\n",
    "X_tfidf = tfidf.fit_transform(train['comment_text'])\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DMBoWPiHlSi",
    "outputId": "4078d1fa-5fe5-4fd9-8d0a-943203e516cd"
   },
   "outputs": [],
   "source": [
    "label_col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "top_features = get_topn_tfidf_feat_byClass(X_tfidf, train[label_col], feature_names, label_col, topn=20)\n",
    "print(top_features['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-Xsv1xAGeg7",
    "outputId": "f0c01553-7edd-4793-9604-71b05a607916"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(train['comment_text'])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Labels\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Get top 20 features per label\n",
    "top_features = get_topn_tfidf_feat_byClass(X_tfidf, train[labels], feature_names, labels, topn=20)\n",
    "\n",
    "# Example: Print top 20 words for \"toxic\"\n",
    "print(top_features['toxic'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0MLvURh9i4Y"
   },
   "source": [
    "# Important unigrams by class visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "3gWf_JrgJarG",
    "outputId": "450af3d9-7dab-41d5-a8c5-e1345e0cf23d"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define label columns\n",
    "label_col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 1), min_df=100,\n",
    "    strip_accents='unicode', analyzer='word',\n",
    "    use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_unigrams = tfidf.fit_transform(train['comment_text'])\n",
    "print(\"Shape of TF-IDF Matrix:\", X_unigrams.shape)\n",
    "print(\"Number of Features:\", len(tfidf.get_feature_names_out()))\n",
    "\n",
    "feature_names = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "# Get top TF-IDF features per class\n",
    "imp_dfs = get_topn_tfidf_feat_byClass(X_unigrams, train[label_col], feature_names, label_col, topn=10)\n",
    "\n",
    "# Plotting - Horizontal colorful bars\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, label in enumerate(label_col):\n",
    "    plt.subplot(3, 2, i + 1)\n",
    "    sns.barplot(\n",
    "        x='tfidf_value',\n",
    "        y='word_feature',\n",
    "        data=imp_dfs[label].head(10),\n",
    "        palette=\"Paired\"  # colorful palette\n",
    "    )\n",
    "    plt.title(f\"Important UniGrams for the class: {label}\", fontsize=12)\n",
    "    plt.xlabel(\"TF-IDF Value\")\n",
    "    plt.ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fw389FRnJprH"
   },
   "source": [
    "few words like fuck seems to be in every class, but again as this is multi-label classification (multiple tags for each comment) there will be that overlapping.\n",
    "\n",
    "especially threat class is standing apart with words like kill, die, death.\n",
    "\n",
    "Interestingly, due to high tf-idf value, word wikipedia has stands in top10 features for the toxic class. which model should not learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3eLFc-X9o7g"
   },
   "source": [
    "#Important bigrams by class visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "-8C90DUIJqwB",
    "outputId": "f02236b8-4a0b-405a-9f25-7a5c6613b666"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# TF-IDF for Bigrams\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(2, 2), min_df=100,\n",
    "    strip_accents='unicode', analyzer='word',\n",
    "    use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_bigrams = tfidf.fit_transform(train['comment_text'])\n",
    "print(\"Shape of TF-IDF Matrix:\", X_bigrams.shape)\n",
    "print(\"Number of Features:\", len(tfidf.get_feature_names_out()))\n",
    "\n",
    "feature_names = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "# Top 10 bigrams per class\n",
    "imp_dfs = get_topn_tfidf_feat_byClass(X_bigrams, train[label_col], feature_names, label_col, topn=10)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, label in enumerate(label_col):\n",
    "    plt.subplot(3, 2, i + 1)\n",
    "    sns.barplot(\n",
    "        x='tfidf_value',\n",
    "        y='word_feature',\n",
    "        data=imp_dfs[label].head(10),\n",
    "        palette=\"Set2\"  # Colorful bars\n",
    "    )\n",
    "    plt.title(f\"Important BiGrams for the class: {label}\")\n",
    "    plt.xlabel(\"TF-IDF Score\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU_OrKNhKOKb"
   },
   "source": [
    "the top 10 tfidf features in case are not much differentiating, every class are having almost same phrases.\n",
    "remember, the comment_text is completely unprocessed for now.\n",
    "\n",
    "There is high chance for Names of persons like famous names george bush, mitt romney (if mentioned repeatedly in the spam) to appear in the bigram features for any class. To avoid this we\n",
    "\n",
    "can preprocess spam comments (comments with < 25% unique words) differently like removing usernames/person names from the comments using regular expressions or POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5A2HlaEKfst"
   },
   "source": [
    "#Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2FSLmxBH2mK"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train['clean_text'] = train['comment_text'].apply(clean_text)\n",
    "test['clean_text']  = test['comment_text'].apply(clean_text)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train['clean_text'],               # <‑‑ now split on cleaned text\n",
    "    train[label_col],\n",
    "    test_size=0.2,\n",
    "    random_state=2019\n",
    ")\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=50_000, ngram_range=(1, 2))\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_val_tf   = tfidf.transform(X_val)\n",
    "X_test_tf  = tfidf.transform(test['clean_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uq9JMicIXZck",
    "outputId": "bf51f1af-a10c-41f7-beb3-ceeff0643859"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Define label columns\n",
    "# label_col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# # Split data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     train['comment_text'],\n",
    "#     train[label_col],\n",
    "#     test_size=0.2,\n",
    "#     random_state=2019\n",
    "# )\n",
    "\n",
    "# X_test = test['comment_text']\n",
    "\n",
    "# print('Train data size:', len(X_train))\n",
    "# print('Validation data size:', len(X_val))\n",
    "# print('Test data size:', len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PpgOALlBKTaM",
    "outputId": "963a6c79-0989-495b-8774-c7ca55969e65"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=9,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_val_tf = tfidf.transform(X_val)\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "\n",
    "print('Final Data dimensions after transformations:', X_train_tf.shape, y_train.shape, X_val_tf.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tknM6fw4M1Gk"
   },
   "source": [
    "Modelling: Baselines\n",
    "\n",
    "Generally, I will begin with least complex models & will move to complex models based on their time/space complexity.\n",
    "\n",
    "1. Naive Bayes\n",
    "Naive Bayes is well know for its Text Sentiment Classification tasks. It is also simple to interpret the model so I am starting with it.\n",
    "\n",
    "If works well, added advantages includes speed - as worst case complexity is O(Nd) is comparatively low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "080yu-x4M2ps",
    "outputId": "f4cda725-a969-4252-dd05-452704175a42"
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "model = OneVsRestClassifier(MultinomialNB(), n_jobs = -1)\n",
    "model.fit(X_train_tf, y_train)\n",
    "print('model: Naive Bayes')\n",
    "print('mean ROC-AUC on train set:', roc_auc_score(y_train, model.predict_proba(X_train_tf)))\n",
    "y_pred_nb = model.predict_proba(X_val_tf)\n",
    "print('mean ROC-AUC on validation set:', roc_auc_score(y_val, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qdgryCQxNQuV",
    "outputId": "828d9bae-7cea-40d6-9cc0-30dac4128af0"
   },
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "\n",
    "train_rocs = []\n",
    "valid_rocs = []\n",
    "\n",
    "preds_train = np.zeros(y_train.shape)\n",
    "preds_valid = np.zeros(y_val.shape)\n",
    "preds_test = np.zeros((len(test), len(label_col)))\n",
    "print('model: Naive Bayes')\n",
    "for i, label_name in enumerate(label_col):\n",
    "    print('\\nClass:= '+label_name)\n",
    "    # fit\n",
    "    model.fit(X_train_tf,y_train[label_name])\n",
    "\n",
    "    # train\n",
    "    preds_train[:,i] = model.predict_proba(X_train_tf)[:,1]\n",
    "    train_roc_class = roc_auc_score(y_train[label_name],preds_train[:,i])\n",
    "    print('Train ROC AUC:', train_roc_class)\n",
    "    train_rocs.append(train_roc_class)\n",
    "\n",
    "    # valid\n",
    "    preds_valid[:,i] = model.predict_proba(X_val_tf)[:,1]\n",
    "    valid_roc_class = roc_auc_score(y_val[label_name],preds_valid[:,i])\n",
    "    print('Valid ROC AUC:', valid_roc_class)\n",
    "    valid_rocs.append(valid_roc_class)\n",
    "\n",
    "    # test predictions\n",
    "    preds_test[:,i] = model.predict_proba(X_test_tf)[:,1]\n",
    "\n",
    "print('\\nmean column-wise ROC AUC on Train data: ', np.mean(train_rocs))\n",
    "print('mean column-wise ROC AUC on Val data:', np.mean(valid_rocs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO5D2NwPNjB3"
   },
   "source": [
    "And the results of both implementations are excatly same..\n",
    "Naive Bayes performed pretty well in its zone with 0.9047...\n",
    "This looks like a good score only until we ecperiment with other models...as they might score a way higher lets see...how linear models like Logistic regression & linear SVM perform with default parameters\n",
    "\n",
    "From here on I will be using the onevsrest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7wbakIUNqUI"
   },
   "source": [
    "2. Logisitc Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIssV419Nm8Y",
    "outputId": "92574ea0-c2cd-42e4-9935-5603e1b2aa80"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = OneVsRestClassifier(LogisticRegression(), n_jobs = -1)\n",
    "model.fit(X_train_tf, y_train)\n",
    "print('model: Logistic Regression')\n",
    "print('mean ROC-AUC on train set:', roc_auc_score(y_train, model.predict_proba(X_train_tf)))\n",
    "y_pred_log = model.predict_proba(X_val_tf)\n",
    "print('mean ROC-AUC on validation set:', roc_auc_score(y_val, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t68cL7zmOCLb"
   },
   "source": [
    "3. Linear SVM\n",
    "\n",
    "Linear SVM will not support probability prediction as hinge loss is unstable, the way around is using a calibration classifer wrapper and it wont support OnevsRest classifers. so we I am using manual python loop again.\n",
    "\n",
    "Notice that I fitted a new CalibratedClassifierCV wrapper afer fitting the LinearSVC classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JGmdSBR7ODrC",
    "outputId": "5c1b1f5c-cc9f-4fd5-b336-69622dae9e3b"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "train_rocs = []\n",
    "valid_rocs = []\n",
    "\n",
    "preds_train = np.zeros(y_train.shape)\n",
    "preds_valid = np.zeros(y_val.shape)\n",
    "preds_test = np.zeros((len(test), len(label_col)))\n",
    "print('model: Linear SVM')\n",
    "for i, label_name in enumerate(label_col):\n",
    "    print('\\nClass:= '+label_name)\n",
    "\n",
    "    # fit\n",
    "    model.fit(X_train_tf,y_train[label_name])\n",
    "\n",
    "    # calibration classifier fit\n",
    "    model = CalibratedClassifierCV(model, cv = 'prefit')\n",
    "    model.fit(X_train_tf, y_train[label_name])\n",
    "\n",
    "    # train\n",
    "    preds_train[:,i] = model.predict_proba(X_train_tf)[:,1]\n",
    "    train_roc_class = roc_auc_score(y_train[label_name],preds_train[:,i])\n",
    "    print('Train ROC AUC:', train_roc_class)\n",
    "    train_rocs.append(train_roc_class)\n",
    "\n",
    "    # valid\n",
    "    preds_valid[:,i] = model.predict_proba(X_val_tf)[:,1]\n",
    "    valid_roc_class = roc_auc_score(y_val[label_name],preds_valid[:,i])\n",
    "    print('Valid ROC AUC:', valid_roc_class)\n",
    "    valid_rocs.append(valid_roc_class)\n",
    "\n",
    "    # test predictions\n",
    "    preds_test[:,i] = model.predict_proba(X_test_tf)[:,1]\n",
    "\n",
    "print('\\nmean column-wise ROC AUC on Train data: ', np.mean(train_rocs))\n",
    "print('mean column-wise ROC AUC on Val data:', np.mean(valid_rocs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLefu9oSOTqU"
   },
   "source": [
    "Performance of Linear Models:\n",
    "\n",
    "Both models Logistic & SVM are performing better than naive bayes models.\n",
    "\n",
    "Although Logistic is better with default parameters, I need to experiment & see if fine tuning helps Linear SVM.\n",
    "\n",
    "Moreover, these models are interpretable & not very complex in terms of time & space required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYIbiPQM9Bt-"
   },
   "source": [
    "##ML experimenting and tracking using weight and biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "w9DyUZazmlhI",
    "outputId": "e7c54593-48b8-4e1d-8626-876275add56c"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hAaZQDe_nyXX",
    "outputId": "c0245c0a-57c9-45cf-8733-7b20aaa84078"
   },
   "outputs": [],
   "source": [
    "# --- Install and import dependencies ---\n",
    "!pip install wandb scikit-learn matplotlib seaborn -q\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# --- Helper function to log metrics and plots ---\n",
    "def log_model_to_wandb(model_name, model, X_train, y_train, X_val, y_val):\n",
    "    # Start a new run\n",
    "    wandb.init(project=\"toxic_comment_classification\", name=model_name)\n",
    "\n",
    "    # Store metrics for each class (multi-label)\n",
    "    class_rocs_train = []\n",
    "    class_rocs_val = []\n",
    "    preds_train = np.zeros(y_train.shape)\n",
    "    preds_val = np.zeros(y_val.shape)\n",
    "\n",
    "    for i, label_name in enumerate(y_train.columns):\n",
    "        # Fit model per class\n",
    "        model.fit(X_train, y_train[label_name])\n",
    "        preds_train[:, i] = model.predict_proba(X_train)[:, 1]\n",
    "        preds_val[:, i] = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        # Compute ROC AUC for train/val\n",
    "        roc_train = roc_auc_score(y_train[label_name], preds_train[:, i])\n",
    "        roc_val = roc_auc_score(y_val[label_name], preds_val[:, i])\n",
    "        class_rocs_train.append(roc_train)\n",
    "        class_rocs_val.append(roc_val)\n",
    "\n",
    "        # Log confusion matrix\n",
    "        cm = confusion_matrix(y_val[label_name], (preds_val[:, i] > 0.5).astype(int))\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", ax=ax, cmap=\"Blues\")\n",
    "        ax.set_title(f\"Confusion Matrix - {label_name}\")\n",
    "        wandb.log({f\"Confusion Matrix/{label_name}\": wandb.Image(fig)})\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Log ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_val[label_name], preds_val[:, i])\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(fpr, tpr, label=f\"{label_name} (AUC={roc_val:.3f})\")\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_title(f\"ROC Curve - {label_name}\")\n",
    "        ax.set_xlabel(\"False Positive Rate\")\n",
    "        ax.set_ylabel(\"True Positive Rate\")\n",
    "        ax.legend()\n",
    "        wandb.log({f\"ROC Curve/{label_name}\": wandb.Image(fig)})\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Log mean ROC AUC\n",
    "    wandb.log({\n",
    "        \"mean_roc_auc_train\": np.mean(class_rocs_train),\n",
    "        \"mean_roc_auc_val\": np.mean(class_rocs_val)\n",
    "    })\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "# --- Run for each model ---\n",
    "# 1. Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "log_model_to_wandb(\"Naive_Bayes\", nb_model, X_train_tf, y_train, X_val_tf, y_val)\n",
    "\n",
    "# 2. Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=500)\n",
    "log_model_to_wandb(\"Logistic_Regression\", lr_model, X_train_tf, y_train, X_val_tf, y_val)\n",
    "\n",
    "# 3. Linear SVM (with calibration for predict_proba)\n",
    "svm_model = CalibratedClassifierCV(LinearSVC())\n",
    "log_model_to_wandb(\"Linear_SVM\", svm_model, X_train_tf, y_train, X_val_tf, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "_B-8WE2GqacZ",
    "outputId": "241395d8-29f4-42b0-e6b4-756a804eb24a"
   },
   "outputs": [],
   "source": [
    "# --- Install dependencies ---\n",
    "!pip install wandb matplotlib seaborn pandas -q\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# --- W&B Project Info ---\n",
    "PROJECT_PATH = \"fareedahab-infineon-technologies/toxic_comment_classification\"\n",
    "\n",
    "# --- Pull Runs from W&B ---\n",
    "api = wandb.Api()\n",
    "runs = api.runs(PROJECT_PATH)\n",
    "\n",
    "# --- Create Leaderboard DataFrame ---\n",
    "data = []\n",
    "for run in runs:\n",
    "    model_name = run.name\n",
    "    auc_train = run.summary.get('mean_roc_auc_train', None)\n",
    "    auc_val = run.summary.get('mean_roc_auc_val', None)\n",
    "    data.append({\"Model\": model_name, \"Train AUC\": auc_train, \"Validation AUC\": auc_val})\n",
    "\n",
    "leaderboard = pd.DataFrame(data)\n",
    "print(\"\\nLeaderboard Table:\\n\", leaderboard)\n",
    "\n",
    "# Save leaderboard as image\n",
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "ax.table(cellText=leaderboard.values, colLabels=leaderboard.columns, loc='center')\n",
    "plt.savefig(\"wandb_leaderboard.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Combined ROC Curves ---\n",
    "def plot_combined_roc(y_val, preds_dict, label_name):\n",
    "    \"\"\"\n",
    "    preds_dict: dictionary of {model_name: predictions_for_class}\n",
    "    y_val: true labels (e.g., y_val['toxic'])\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for model_name, preds in preds_dict.items():\n",
    "        fpr, tpr, _ = roc_curve(y_val, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"{model_name} (AUC={roc_auc:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title(f\"ROC Curve - {label_name}\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f\"roc_combined_{label_name}.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example for the \"toxic\" class:\n",
    "# preds_dict = {\n",
    "#     \"Naive Bayes\": preds_val_nb[:, 0],\n",
    "#     \"Logistic Regression\": preds_val_lr[:, 0],\n",
    "#     \"Linear SVM\": preds_val_svm[:, 0]\n",
    "# }\n",
    "# plot_combined_roc(y_val['toxic'], preds_dict, label_name=\"toxic\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
